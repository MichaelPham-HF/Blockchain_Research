{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL SCRAPE FOR BOTH VNEXPRESS AND VIETNAMNET\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "http = httplib2.Http()\n",
    "#You paste the link of the news outlet that enumerates the list of articles for searching keywords.\n",
    "#For VIETNAMNET, you can add a variable (e.g. \"crypto_keyword_url\") and do the same thing as below.\n",
    "blockchain_keyword_url = 'https://timkiem.vnexpress.net/?q=blockchain&media_type=all&fromdate=0&todate=0&latest=on&cate_code=&search_f=title,tag_list&date_format=all&page='\n",
    "bitcoin_keyword_url = 'https://timkiem.vnexpress.net/?q=bitcoin&media_type=all&fromdate=0&todate=0&latest=on&cate_code=&search_f=title,tag_list&date_format=all&page='\n",
    "\n",
    "blockchain_list_url = []\n",
    "bitcoin_list_url = []\n",
    "\n",
    "for i in range(12):\n",
    "  blockchain_list_url.append(blockchain_keyword_url + str(i+1))\n",
    "\n",
    "for i in range(26):\n",
    "  bitcoin_list_url.append(bitcoin_keyword_url + str(i+1))\n",
    "\n",
    "def web_scrape(url):\n",
    "  status, response = http.request(url)\n",
    "  for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "      if link.has_attr('href'):\n",
    "          if link['href'].endswith(\"html\"):\n",
    "            print(f\"{link['href']}\")\n",
    "\n",
    "for i in range(12):\n",
    "  web_scrape(blockchain_list_url[i]) \n",
    "for i in range(26):\n",
    "  web_scrape(bitcoin_list_url[i]) \n",
    "\n",
    "#WARNING: the outputs are filled with irrelevant links. Manual check is required to obtain the correct article links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VNEXPRESS TITLES EXTRACTION\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"...csv\") #obtained after applying URL scrape code\n",
    "\n",
    "def fetch_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_tag = soup.find(\"title\")\n",
    "        return title_tag.text.strip() if title_tag else \"No title found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching title\"\n",
    "\n",
    "titles = []\n",
    "\n",
    "for i, url in enumerate(df['url']):\n",
    "    print(f\"Fetching {i+1}/{len(df)}: {url}\")\n",
    "    title = fetch_title(url)\n",
    "    titles.append(title)\n",
    "    time.sleep(1) #Delay to avoid server overload\n",
    "\n",
    "df.insert(0, 'title', titles)\n",
    "df.to_csv(\"...csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VNEXPRESS CONTENT SCRAPE\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"...csv\") #obtained after applying the TITLE EXTRACTION code\n",
    "\n",
    "def extract_vnexpress_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        meta_tag = soup.find(\"meta\", attrs={\"name\": \"pubdate\"})\n",
    "        if meta_tag and meta_tag.get(\"content\"):\n",
    "            raw_date = meta_tag[\"content\"][:10]  # e.g. '2025-06-20'\n",
    "            pub_date = datetime.strptime(raw_date, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")\n",
    "        else:\n",
    "            pub_date = \"No date\"\n",
    "\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        content_clean = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "\n",
    "        return pub_date, content_clean\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error\", f\"Error: {str(e)}\"\n",
    "\n",
    "dates, contents = [], []\n",
    "for i, url in enumerate(df[\"url\"]):\n",
    "    print(f\"Fetching {i+1}/{len(df)}: {url}\")\n",
    "    date, content = extract_vnexpress_data(url)\n",
    "    dates.append(date)\n",
    "    contents.append(content)\n",
    "    time.sleep(1) #Delay to avoid server overload\n",
    "\n",
    "df[\"date\"] = dates\n",
    "df[\"content\"] = contents\n",
    "df.to_csv(\"...csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff84938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUOITRE\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "#All the links were manually collected and saved in a CSV. No beautiful soup scraping here.\n",
    "df = pd.read_csv(\"...csv\")\n",
    "\n",
    "def extract_date(soup):\n",
    "    date_div = soup.find(\"div\", attrs={\"data-role\": \"publishdate\"})\n",
    "    if date_div:\n",
    "        raw = date_div.text.strip()\n",
    "        try:\n",
    "            dt = datetime.strptime(raw.split(\" \")[0], \"%d/%m/%Y\")\n",
    "            return dt.strftime(\"%d-%m-%Y\")\n",
    "        except:\n",
    "            return \"Unrecognized format\"\n",
    "    return \"No date\"\n",
    "\n",
    "def extract_content(soup):\n",
    "    content_div = soup.find(\"div\", class_=\"detail-content\")\n",
    "    if content_div:\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        content_raw = \" \".join(p.get_text(separator=\" \", strip=True) for p in paragraphs)\n",
    "        content_clean = re.sub(r'\\s+', ' ', content_raw).strip()\n",
    "        return content_clean\n",
    "    return \"No content\"\n",
    "\n",
    "dates, contents = [], []\n",
    "for i, url in enumerate(df[\"url\"]):\n",
    "    print(f\"Fetching {i+1}/{len(df)}: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        date = extract_date(soup)\n",
    "        content = extract_content(soup)\n",
    "        dates.append(date)\n",
    "        contents.append(content)\n",
    "    except Exception:\n",
    "        dates.append(\"Error\")\n",
    "        contents.append(\"Error\")\n",
    "    time.sleep(1) #Delay\n",
    "\n",
    "df[\"date\"] = dates\n",
    "df[\"content\"] = contents\n",
    "df.to_csv(\"tuoitre_update.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VNECONOMY\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "#All the links were manually collected and saved in a CSV \n",
    "\"\"\"\n",
    "Tips:\n",
    "Apparently, the links of VNECONOMY articles can be derived\n",
    "from merging all the words from the titles,\n",
    "except that certain special characters are re-converted\n",
    "to different forms (e.g., \".\" -> \"-\").\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"...csv\")\n",
    "\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        content_div = soup.find(\"div\", class_=\"detail__content\")\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all(\"p\")\n",
    "            content_raw = \" \".join(p.get_text(separator=\" \", strip=True) for p in paragraphs)\n",
    "            content_clean = re.sub(r'\\s+', ' ', content_raw).strip()\n",
    "            return content_clean\n",
    "        return \"No content found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "contents = []\n",
    "for i, url in enumerate(df[\"url\"]):\n",
    "    print(f\"Fetching {i+1}/{len(df)}: {url}\")\n",
    "    content = extract_content(url)\n",
    "    contents.append(content)\n",
    "    time.sleep(1) #delay \n",
    "\n",
    "df[\"content\"] = contents\n",
    "df.to_csv(\"example.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebe3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIETNAMNET\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"...csv\") #obtained after applying URL scrape code\n",
    "\n",
    "def get_time(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        time_div = soup.find(\"div\", class_=\"bread-crumb-detail__time\")\n",
    "        if time_div:\n",
    "            raw_date = time_div.text.strip().split(\",\")[-1].strip()  # e.g., '14/07/2025 - 14:38'\n",
    "            date_part = raw_date.split(\"-\")[0].strip()  # get only the date part '14/07/2025'\n",
    "            pub_date = datetime.strptime(date_part, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            return pub_date\n",
    "        else:\n",
    "            return \"No date found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        content_clean = re.sub(r'\\s+', ' ', article_text).strip()\n",
    "        return content_clean\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title_tag = soup.find(\"title\")\n",
    "        return title_tag.text.strip() if title_tag else \"No title found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "titles = []\n",
    "dates = []\n",
    "contents = []\n",
    "\n",
    "for i, url in enumerate(df[\"url\"]):\n",
    "    print(f\"Fetching {i+1}/{len(df)}: {url}\")\n",
    "    titles.append(get_title(url))\n",
    "    dates.append(get_time(url))\n",
    "    contents.append(get_content(url))\n",
    "    time.sleep(1) ##Delay to avoid server overload\n",
    "\n",
    "df[\"title\"] = titles\n",
    "df[\"date\"] = dates\n",
    "df[\"content\"] = contents\n",
    "df = df[[\"title\", \"url\", \"date\", \"content\"]] \n",
    "\n",
    "df.to_csv(\"BITCOIN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efafe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VNECONOMY scrape for \"No_content\", \"Error\", skipped rows\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"example.csv\")\n",
    "\n",
    "def fetch_vneconomy_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Div for VnEconomy content\n",
    "        content_div = soup.find(\"div\", class_=\"detail__content\")\n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all([\"p\", \"div\", \"br\"])\n",
    "            text = \" \".join(content_div.stripped_strings)\n",
    "            return text.strip()\n",
    "\n",
    "        return \"No content found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "df[\"content\"] = df[\"url\"].apply(lambda x: fetch_vneconomy_content(x))\n",
    "df.to_csv(\"example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8481c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning PIPELINES\n",
    "# The following code might not apply to every source and provide a general framework.\n",
    "# It depends on the specific issues of a dataset. \n",
    "\n",
    "# The code is designed to handle common problems like:\n",
    "#   - removing quotation marks\n",
    "#   - filtering out error messages\n",
    "#   - collecting rows with missing content for potential re-scraping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"...csv\") \n",
    "# language_content = \"content\" #for Vietnamese content || #\"content_en\" #for English content\n",
    "\n",
    "def quotation_mark_removal(df, language_content):\n",
    "    df[\"title\"] = df[\"title\"].astype(str).str.replace(r\"[\\\"']\", \"\", regex=True)\n",
    "    df[language_content] = df[language_content].astype(str).str.replace(r\"[\\\"']\", \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "def filter_content(df, language_content):\n",
    "    # df= df.drop(columns=[\"Unnamed: 3\"])\n",
    "    df = df[df[language_content] != \"error\"]\n",
    "    df = df[df[language_content] != \"No content\"]\n",
    "    df = df[df[language_content] != \"No content found\"]\n",
    "    df = df[df[language_content] != \"Error: Invalid URL error: No scheme supplied. Perhaps you meant https://error?\"]\n",
    "    df = df[df[language_content] != \"Error: Invalid URL Error: No Scheme Supplied. Perhaps You MeanT https: // Error?\"]\n",
    "    df = df[df[\"url\"] != \"error\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "#SPECIAL ISSUES\n",
    "#Sometimes the content contains double quotation marks that render the text analysis malfunctional.\n",
    "#The following function replaces any \"text\" with 'text'\n",
    "def replace_quotes(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Replace any \"text\" with 'text'\n",
    "    return re.sub(r'\"(.*?)\"', r\"'\\1'\", text)\n",
    "\n",
    "df['content'] = df['content'].apply(replace_quotes)\n",
    "\n",
    "\n",
    "#Collecting rows with 'No content found' to re-scrape for contents\n",
    "def no_content_df_collection(df, language_content): \n",
    "    no_content_df = df[df[language_content] == 'No content found']\n",
    "    return df\n",
    "\n",
    "df = quotation_mark_removal(df, )\n",
    "\n",
    "df = filter_content(df, )\n",
    "\n",
    "no_content_df_collection(df, )\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"title\"])\n",
    "df.to_csv(\"...csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
