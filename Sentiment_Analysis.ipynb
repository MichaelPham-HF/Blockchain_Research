{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the CSV inot smaller batch sizes for sentiment analysis or analysis with OPENAI API\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"...csv\")\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 300 #Adjust this based on your needs\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[i:i+batch_size]\n",
    "    batch_df.to_csv(f\"...{i+1}-{i+len(batch_df)}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f356170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment - CryptoBERT\n",
    "from transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"ElKulako/cryptobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, max_length=64, truncation=True, padding='max_length')\n",
    "\n",
    "# Load CSVs\n",
    "df_1 = pd.read_csv(\"...csv\") #vietnamese versions\n",
    "#...\n",
    "df_a = pd.read_csv(\"...csv\") #english versions\n",
    "#...\n",
    "\n",
    "def sentiment(filename, language_content):\n",
    "  # Create new columns for sentiment and confidence\n",
    "  filename['sentiment'] = None\n",
    "  filename['confidence'] = None\n",
    "  for i, content in enumerate(filename[language_content]):\n",
    "    if content != \"No content\":\n",
    "      result = pipe(content)[0]\n",
    "      # title = filename[\"title\"][i]\n",
    "      # print(\"\")\n",
    "      # print(f\"Title: {title}\")\n",
    "      sentiment = result['label']\n",
    "      confidence = round(result['score'], 4)\n",
    "      filename.loc[i, 'sentiment'] = sentiment\n",
    "      filename.loc[i, 'confidence'] = confidence\n",
    "      # print(f\"Sentiment: {sentiment}, Confidence: {confidence:.4f}\")\n",
    "  return filename\n",
    "\n",
    "df_analyzed_1 = sentiment(df_1, \"content\") # Vietnamese csvs\n",
    "df_analyzed_1.to_csv(\"...csv\", index=False)\n",
    "#...\n",
    "\n",
    "df_analyzed_a = sentiment(df_a, \"content_en\") # English csvs\n",
    "df_analyzed_a.to_csv(\"...csv\", index=False)\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment - FinBERT\n",
    "from transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "pipe_finbert = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Load CSVs\n",
    "df_1 = pd.read_csv(\"...csv\") #Vietnamese CSVs\n",
    "#...\n",
    "df_a = pd.read_csv(\"...csv\") #English CSVs\n",
    "#...\n",
    "\n",
    "def sentiment(df, language_content):\n",
    "  df['sentiment'] = None\n",
    "  df['weight'] = None\n",
    "\n",
    "  for i, content in enumerate(df[language_content]):\n",
    "    if isinstance(content, str) and content.strip():\n",
    "      result = pipe_finbert(content)[0]\n",
    "      best = max(result, key=lambda r: r['score'])\n",
    "\n",
    "      df.at[i, 'sentiment'] = best['label']\n",
    "      df.at[i, 'weight'] = round(best['score'], 4)\n",
    "    else:\n",
    "      df.at[i, 'sentiment'] = 'unknown'\n",
    "      df.at[i, 'weight'] = 0.0\n",
    "\n",
    "  return df\n",
    "\n",
    "df_analyzed_1 = sentiment(df_1, \"content_en\") #English CSVs\n",
    "df_analyzed_1.to_csv(\"sentiment_tuoitre_bit_en.csv\", index=False)\n",
    "#...\n",
    "df_analyzed_a = sentiment(df_a, \"content\") #Vietnamese CSVs\n",
    "df_analyzed_a.to_csv(\"sentiment_tuoitre_block_vn.csv\", index=False)\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment - Wonrax PhoBERT\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"wonrax/phobert-base-vietnamese-sentiment\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "labels = [\"negative\", \"positive\", \"neutral\"]\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    try:\n",
    "        segmented = ViTokenizer.tokenize(text)\n",
    "        inputs = tokenizer(segmented, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = logits.softmax(dim=-1).tolist()[0]\n",
    "            max_idx = int(torch.argmax(logits))\n",
    "            return labels[max_idx], round(probs[max_idx], 4)\n",
    "    except:\n",
    "        return \"ERROR\", 0.0\n",
    "\n",
    "def process_csv(input_path, output_path, content_column=\"content\"):\n",
    "    df = pd.read_csv(input_path)\n",
    "    sentiments, scores = [], []\n",
    "    for content in df[content_column]:\n",
    "        if isinstance(content, str) and content.strip() and content != \"No content\":\n",
    "            label, score = classify_sentiment(content)\n",
    "        else:\n",
    "            label, score = \"No content\", 0.0\n",
    "        sentiments.append(label)\n",
    "        scores.append(score)\n",
    "    df[\"sentiment\"] = sentiments\n",
    "    df[\"score\"] = scores\n",
    "    df.to_csv(output_path, index=False)\n",
    "    # print(f\"Processed: {input_path} into {output_path}\")\n",
    "\n",
    "# Vietnamese CSVs\n",
    "process_csv(\"...\", \"...\", content_column=\"content\")\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43dcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment - Mr4 PhoBERT\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or text.strip().lower() in [\"\", \"no content\"]:\n",
    "            return \"No content\", 0.0\n",
    "        inputs = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "        label_id = torch.argmax(probs).item()\n",
    "        label = model.config.id2label[label_id] if hasattr(model.config, \"id2label\") else f\"LABEL_{label_id}\"\n",
    "        return label, round(probs[label_id].item(), 4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at text: {text[:50]}... | {e}\")\n",
    "        return \"ERROR\", 0.0\n",
    "\n",
    "def process_csv(input_path, output_path, content_column=\"content\"):\n",
    "    df = pd.read_csv(input_path)\n",
    "    df[\"sentiment\"] = \"\"\n",
    "    df[\"score\"] = 0.0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sentiment, score = classify_sentiment(row[content_column])\n",
    "        df.at[i, \"sentiment\"] = sentiment\n",
    "        df.at[i, \"score\"] = score\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n",
    "    # print(f\"Processed: {input_path} into {output_path}\")\n",
    "\n",
    "# Vietnamese CSVs\n",
    "process_csv(\"...csv\", \"...csv\")\n",
    "#..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
